#!/bin/bash
#SBATCH --output=Grom.out
#SBATCH --error=Grom.err
#SBATCH --job-name=NTCDA10_Li1
#SBATCH --time=200:00:00 
#SBATCH --clusters=chemistry
##SBATCH --clusters=ub-hpc 
#SBATCH --partition=beta --qos=beta
##SBATCH --partition=general-compute
#SBATCH --account=hachmann
#SBATCH --nodes=1
#SBATCH --tasks-per-node=16
##SBATCH --cluster=CHEMISTRY
#SBATCH --constraint="CPU-E5-2630v3"
##SBATCH --exclude=cpn-p25-18,cpn-p25-19
##SBATCH --exclusive

cur_dir=$(pwd)
cd //gpfs/scratch/kmukherj/Li_small/

if [ -e NTCDA10_Li1 ]; then
rm -r NTCDA10_Li1
fi 
mkdir -p NTCDA10_Li1
cd NTCDA10_Li1
cp $cur_dir/inp_ut.inp ./
cp $cur_dir/*.xyz ./

INFILE=inp_ut.inp 

if [ ! -f $INFILE ]; then
  echo "Error! Input file does not exist (${INFILE})"
  echo "Aborting the job..."
  exit
fi

BASE=`basename $INFILE .inp`

# a parallel version of the infile
PINFILE=${BASE}.pin

#construct nodefile, named as per Orca manual
export SLURM_NODEFILE=${BASE}.nodes

#use analogous naming convention for rankfile
export ORCA_RANKFILE=${BASE}.ranks

tic=`date +%s`
echo "Start Time = "`date`

# load modules
echo "Loading modules ..."
module load openmpi/gcc-4.8.3/1.6.5
module load gromacs/5.1.1
ulimit -s unlimited
module list

# disable PSM interface if the nodes have Mellanox hardware
bMLX=`/usr/sbin/ibstat | head -n1 | grep mlx | wc -l`
if [ "$bMLX" == "1" ]; then
  export OMPI_MCA_mtl=^psm
fi

# change to working directory
#cd $SLURM_SUBMIT_DIR

echo "%pal nprocs $SLURM_NPROCS end" > $PINFILE
cat $INFILE | grep -v "^%pal nprocs" >> $PINFILE

echo "Launching orca ..."
echo "SLURM job ID         = "$SLURM_JOB_ID
echo "Working Dir          = "$SLURM_SUBMIT_DIR
echo "Compute Nodes        = "`nodeset -e $SLURM_NODELIST`
echo "Number of Processors = "$SLURM_NPROCS
echo "Number of Nodes      = "$SLURM_NNODES 
echo "mpirun command       = "`which mpirun`
echo "orca nodefile        = "$SLURM_NODEFILE
echo "orca rankfile        = "$ORCA_RANKFILE

echo " "
echo "Input file"
cat $PINFILE
echo " "

# create rank file to explicitly bind cores
echo "creating hostfile and rankfile"
uid=`id -u`
jid=$SLURM_JOB_ID
nodes=`nodeset -e $SLURM_NODELIST`

# trigger creation of cpuset information and save to working dir
srun bash -c "cat /cgroup/cpuset/slurm/uid_${uid}/job_${jid}/cpuset.cpus > cpus.\`hostname\`.$SLURM_JOB_ID"

rm -f $ORCA_RANKFILE
rm -f $SLURM_NODEFILE
rank=0
for i in ${nodes}; do
  # extract space-separated list of assigned cpus
  cpus=`cat cpus.${i}.${SLURM_JOB_ID}`
  cpus=`nodeset -Re $cpus`
  # add cpu assignments to the rank file
  for j in ${cpus}; do
    echo "rank ${rank}=$i slot=$j" >> $ORCA_RANKFILE
    echo "$i" >> $SLURM_NODEFILE
    rank=`expr $rank + 1`
    if [ "$rank" == "$SLURM_NPROCS" ]; then
      break;
    fi
  done
  if [ "$rank" == "$SLURM_NPROCS" ]; then
    break;
  fi
done

# use ssh instead of slurm as the launcher
# the rankfile that was just created will ensure cpusets are still honored.
export OMPI_MCA_plm=rsh

# launch application using mpirun
echo "Launching application using mpirun"

# launch application
$ORCA_PATH/orca $PINFILE

echo "All Done!"

echo "End Time = "`date`
toc=`date +%s`

elapsedTime=`expr $toc - $tic`
echo "Elapsed Time = $elapsedTime seconds"

#copying back relavant files to current directory
cp *.gbw $cur_dir/. 
cp *.xyz $cur_dir/. 
cp *.trj $cur_dir/.
